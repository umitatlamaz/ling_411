# Effect Size & Significance
Often times, we will observe a difference between the means of two groups. These groups could be variables of any factor. For example, age could be a factor an we are investigating the difference in voice pitch between children and adults. Or, we could be looking at the difference between the frequency of using the word **darlamak** between members of Gen X and Gen Z. Yet another example would be looking at the difference between the average valence of smell and taste words.

In all these cases, we are comparing two means. So, let's say we collect data from 100 adults and 100 children and observe that on average children's fundamental frequencies in their voice pitch is 100Hz higher than those of adults. 

What have we found? Is this a big difference? (As humans we are usually able to distinguish between an adult and a child when they speak. So, assuming that fundamental frequency is a predictor of adult vs child, then we know that some level of difference in Hertz is the source of this difference. But what is that level?)

Let's say we run another experiment with 5 children and 5 adults. Now, we find the average difference to be 200. So, once again, we face the same question. Is 100 big? Is 200 big? Obviously 200 is bigger than 100 but we don't know if these are big numbers or not. 

Once again, we need to think of magnitude not in terms of absolute values but relative to a population (e.g. humans, children, adults, men, women, etc.). So, it would be great if we could figure out a way to calculate the **effect size** of a predictor (e.g. being adult or being a child) in a **standardized way**. If we have a standard definition of effect size, then we can compute this for any kind of metric.

As we are trying to figure out a standardized effect size (and other values), we'll keep three main points in mind:

* **Magnitude of difference**
  * The bigger the difference between (means of) two sample groups, the more you should expect to see a difference in the population. 
  
* **Variability in the data**
  * The less variability in the data, the more certain you'll be about the estimate. 
  
* **Sample Size**
  * The bigger the sample size, the more accurate is your measurement of the difference.


## Cohen's *d*
Cohen's *d* is the measure used to quantify the **strength** of a difference between two means (m1 and m2). The formula for Cohen's *d* is given below:

$$d = \frac{m_1-m_2}{s}$$
where: 
$$m_1 = \text{mean of group 1}$$
$$m_2 = \text{mean of group 2}$$

$$s = \text{pooled standard deviation}$$

We'll calculate s using the formula below for **pooled** standard deviation:

$$s = \sqrt{\frac{(n_1-1)*sd_1^2 + (n_2-1)*sd_2^2}{n_1+n_2-2}}$$

where:

$$n_1 = \text{number of items in group 1}$$

$$n_2 = \text{number of items in group 2}$$

$$sd_1 = \text{standard deviation of group 1}$$

$$sd_2 = \text{standard deviation of group 2}$$



The reason why we are calculating the **pooled** standard deviation is to make sure that our standard deviations that come from different means (i.e. groups of data) are weighted. Below, we'll see that the pooled standard deviation has a slightly different value than the standard deviation of the whole data. 



**Let us try Cohen's *d* for the Smell and Taste data. **


First, load the data and select the relevant data using the filter function. 
```{r message=FALSE}
#Import tidyverse
library(tidyverse)
#Read in the data
data <- read_csv('data/winter_2016_senses_valence.csv')
#filter in the data for the relevant conditions (Taste and Smell)
senses_data <- filter(data, Modality %in% c('Taste', 'Smell'))
#print the head to see what it looks like
data
```

Let us get the means for each category. 

```{r}
#calculate means for each condition (Taste, Smell)
means <- senses_data %>% 
  group_by(Modality) %>% 
  summarize(avg = mean(Val))

#print means to see what it looks like
means
```

Let us get the length of each group (i.e. number of items in each group).

```{r}
#get the number of items for each condition (Taste, Smell)
lengths  <- senses_data %>% 
  group_by(Modality) %>% 
  summarize(N = n())

```




Let us now calculate s (pooled standard deviation).

```{r}
#calculate standard deviation for each condition (Taste, Smell)
s_devs  <- senses_data %>% 
  group_by(Modality) %>% 
  summarize(s = sd(Val))

#calculate pooled standard deviation using the formula above
s <- sqrt(((lengths$N[1]-1)*(s_devs$s[1]^2) + (lengths$N[2]-1)*(s_devs$s[2]^2))/(lengths$N[1]+lengths$N[2]-2))

#print the pooled standard deviation
s

#calculate and print the standard deviation of the whole data for comparison
sd(senses_data$Val)
```



Now we can calculate Cohen's *d*.

```{r}
d = (means$avg[1] - means$avg[2]) / s
d
```

We can also use the package `effsize` to calculate Cohen's d. You'll see observe a difference between our calculation and the calculation given by the effsize package This is mainly because they use a slightly difference way of calculating variation in the data (the denominator). That's not a huge difference. 

```{r}
library(effsize)
cohen.d(Val ~ Modality, data = senses_data)
```

How to interpret Cohen's d?

|**Cohen's *d* **| **Magnitude** | 
|:--|:--|
| \|0.2\| | small | 
| \|0.5\| | medium | 
| \|0.8\| | large | 



```{r cohens, echo=FALSE, fig.cap="Cohen's *d*."}
knitr::include_graphics("./img/cohens.png")
```

## Standard Error
Cohen's *d* is a standard measure of difference in magnitude between two samples. It does not care about the sample size though. Your sample size for each group could be 2, 200, 2K, 2M, and so on. 

Although, Cohen's *d* can tell us whether the difference between the two samples is large, it won't tell us much about our population. 

Here's a small example. I want to test if there is a difference between the voice pitch of people who wear **blue shirts** and people who wear **red shirts**. Assume that the following is my data. 

|**Participant (by shirt color) **| **Fundamental Frequency** | 
|:--|:--|
| Blue 1 | 175 | 
| Blue 2 | 125 | 
| Red 1 | 220 | 
| Red 2 | 190 | 


Let us run Cohen's d on this data to see what the magnitude of the difference is. 

```{r}
blue = c(175,125)
red = c(220,190)
mean(blue) - mean(red)
cohen.d(blue,red)
```

We get a large effect. However, there's no way we can conclude that shirt color has anything to do with someone's voice pitch. We cannot generalize to the population. We observe a difference but we probably have a decent error margin in our prediction. To quantify this error, we need to introduce a new metric **standard error**.

Standard Error (SE) is a combination of the **variability** in the data and the **sample size**

$$SE = \frac{s}{\sqrt{N}}$$

* s = standard deviation
* N = sample size

The bigger the standard error, the less accurate is your estimation of the population parameters. This means your estimation (of the parameters, i.e. mean and standard deviation) is less reliable. The smaller the SE, the more accurate is your calculation of the population parameter estimates. 

As you can see from the formula:

* SE will increase as your standard deviation grows (i.e. there is more variance in the data)
* SE will decrease as your sample size grows


Let us calculate the standard error for each of our means.

```{r}
#Calculate Standard Errors
SE_blue <- sd(blue)/sqrt(2)
SE_red <- sd(red)/sqrt(2)

#print Standard Error for blue shirts
SE_blue

#print Standard Error for red shirts
SE_red
```


Standard Error tells us how close or far away from the true population mean. In this example, we were trying to get the average voice pitch for people wearing a blue shirt and people wearing a red shirt. In each sample, we have only 2 samples. Obviously, this is very little data and the Standard Errors are going to be big. 

## Confidence Interval

Confidence interval is a metric that helps you determine the level of confidence in your population parameter estimates. The formula for 95% Confidence Interval is as follows:

$$CI = [mean \pm 1.96 * SE ]$$
Let us decompose this formula a bit. 

* **mean** is the sample mean of some sample
* **SE** is the standard error of the sample
* $\pm$ indicates that the value is going to be somewhere between *mean plus or minus some value*.
* **1.96** is a special number that indicates the 95% CI.[^1] 

[^1]: In fact, it corresponds to the z-score value of 95%. For calculating the 90% interval, we would use the corresponding z-score value 1.64. 


As the name suggests, CI is an interval and the numbers returned define the range of possible values for 95% of the time. 

Let us calculate the 95% confidence intervals for our our means for the blue and red data.

```{r}
#calculate means
mean_blue <- mean(blue)
mean_red <- mean(red)


#calculate CIs
CI_blue <- c(mean_blue - 1.96 * SE_blue, mean_blue + 1.96 * SE_blue)
CI_red <- c(mean_red - 1.96 * SE_red, mean_red + 1.96 * SE_red)


name <-c('mean','SE','CI_min','CI_max')
val_blue<-c(mean_blue,SE_blue,CI_blue)
val_red<-c(mean_red,SE_red,CI_red)

#print mean, SE, and CI for blue
blue_st <- data.frame(name, val_blue)
blue_st

#print mean, SE, and CI for red
red_st <- data.frame(name, val_red)
red_st
```

## Standard Error of the difference of two means
In the previous sections, we calculated the SE and CI for each sample (blue and red). However, what we are ultimately interested in is the difference in the means between two groups. The difference in the mean is what will tell us if there is a significant difference between the voice pitches of people wearing different colors. To calculate the Standard Error for the difference in mean of two samples, we use the formula below. 

$$SE_{diff} = \sqrt{\frac{SD_1^2}{n_1}+\frac{SD_2^2}{n_2}}$$
Let us calculate the SE of the difference in mean. 

```{r}
#calculate standard deviations

sd_blue <- sd(blue)
sd_red <- sd(red)

# calculate SE_diff
SE_diff <- sqrt(sd_blue^2/2 + sd_red^2/2)

#print SE_diff
SE_diff
```

Let us now calculate the 95% CI for the difference in means. 

```{r}
#calculate the difference in means
diff_mean <- mean_blue - mean_red

#calculate the 95% CI
CI_diff <- c(diff_mean-1.96*SE_diff, diff_mean+1.96*SE_diff)

#print the CI_diff by rounding the numbers to 2 decimal point
round(CI_diff, 2)
```


## Hypothesis Testing
Remember that throughout the semester we talked about forming and testing hypotheses. Let us form our hypotheses and then test them.

Let us build our **alternative hypothesis**.

* H1: There is a difference in voice pitch between people who wear blue and people who wear red.

Now, let us build our null hypothesis. 

* H0: There is **no** difference in voice pitch between people who wear blue and people who wear red.


In hypothesis testing framework, we do not try proving our alternative hypothesis (H1). Instead, we try **rejecting the null hypothesis**.

To reject the null hypothesis we do the following:

1. Make an observation. 
  * In this case, our observation is the difference in means between two colors. 
2. Calculate the probability of making this observation. 
  * This means we need to find the **p-value** of the observation.
3. Check the **p-value** against a **critical value** called alpha ($\alpha$).
  * If the **p-value** is smaller than the **critical value**, then we **reject** the null hypothesis.
  * else, we maintain the null hypothesis. 
  
  
What is the **critical value**. Critical value is a value that we define depending on the nature of our question. It's up to us and how we want to interpret the results. The scientific community has converged on using several critical values.

 * $\alpha$ = 0.05 (commonly adopted for social sciences)
 * $\alpha$ = 0.01
 * $\alpha$ = 0.001


**So, how do we calculate the p-value for our null hypothesis**?

In other words, how do we calculate the p-value for the difference we observe between people who wear blue and red? To do this, we need a particular method that allows us to calculate some statistics for the **difference between two means**. For this, we will use a **t-test** which allows us to calculate a standardized **t-score** which comes from a particular distribution called **t-distribution**. Once we have a **t-score**, we can check it against the **t-distribution** to calculate its **p-value**. 





## Calculating the t-score

**t-score** is calculated using the following formula. 

$$t = \frac{mean1-mean2}{SE_{diff}}$$
What the t-score encodes is 1) magnitude of the difference, 2)Variability in the data 3) Sample size. So, all of the information we need.

Let us calculate the t-score for our data. 

```{r}
#calculate the t-score
t <- diff_mean / SE_diff
#print the t-score
t
```

**How do we interpret the t-score**
t-score is going to be a value from a t-distribution. A t-distribution is very similar to a normal distribution except that it has **heavier tails**. What this means is that there is a higher probability of having more extreme values especially with small data sets. Let's take a look at the t-and normal distributions. 


A t-distribution is like a normal distribution but has an additional parameter called **degrees of freedom**. This is relatively vague concept we won't go into. You can google it and find what it means intuitively. For our purposes, we will just assume that degrees of freedom is calculated by deducing 1 from our sample size. 

 * df_blue = sample_size_blue - 1
 * df_red = sample_size_red - 1
 
 When we are calculating the degrees of freedom for both samples, then we calculate it using the formula below:
 
 * df = sample_size_blue + sample_size_red -2
 
Given that we have a total of 4 data points, then our df will be 2.
 

```{r}
curve(dt(x, df=2), from=-4, to=4, col = 'red',ylim=c(0,0.5)) # 2 degrees of freedom
curve(dnorm(x), from=-4, to=4, col = 'steelblue',add=TRUE) 

legend(-4, .3, legend=c("t (df=2)", "normal"),
       col=c("red", "steelblue" ), lty=5, cex=1.2)
```


**t-distribution** approaches normal distribution (for z-scores) as the degrees of freedom increases. At 30 degrees of freedom or above, it becomes very similar to normal distribution.[^2]

[^2]: This is part of the reason why people aim for at least 30 participants in each group. 


```{r}
curve(dt(x, df=10), from=-4, to=4, col = 'red',ylim=c(0,0.5)) #10 degrees of freedom
curve(dnorm(x), from=-4, to=4, col = 'steelblue',add=TRUE) 

legend(-4, .3, legend=c("t (df=2)", "normal"),
       col=c("red", "steelblue" ), lty=5, cex=1.2)
```

```{r}
curve(dt(x, df=30), from=-4, to=4, col = 'red',ylim=c(0,0.5)) #30 degrees of freedom
curve(dnorm(x), from=-4, to=4, col = 'steelblue',add=TRUE) 

legend(-4, .3, legend=c("t (df=2)", "normal"),
       col=c("red", "steelblue" ), lty=5, cex=1.2)
```


## p-value
p-value is the probability of observing a particular t-score given a t-distribution. So, all we need to do now is to calculate the p-value for the t-score we observed above. There are various ways of doing this. Many people use a t-table to do the calculation. See [this link](https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf).

Alternatively, we can calculate it using the `pt()` function, which takes the following arguments.

* t-score
* degrees of freedom
* A Boolean for lower.tail (optional)

```{r}
pt(abs(t),1.6374,lower.tail = FALSE)
```

The value we get is going to be the probability of t-score on one tail of the distribution (called **one-tailed**). This is used for a directional hypothesis (e.g. red shirts have a higher voice pitch than blue ones). In our case, we are interested in the difference regardless of the direction. For this, we can simply multiply our p-value by 2 to get a **two-tailed** distribution. 

```{r}
2*pt(abs(t),1.6374, lower.tail = FALSE)
```


It looks like our p-value is larger than the critical value $\alpha = 0.05$. So, we must **cannot reject** our null hypothesis. This means that, there is a big chance of observing such a difference in mean when we have only a total of 4 data points. 

Even though our effect size is large as can be seen below, our results are **not significant**. In other words, the big difference is due to chance and we cannot conclude that there is a difference in the voice pitches of people who wear blue and people who wear red.


```{r}
cohen.d(blue,red)
```


## Type I and Type II Errors
Sometimes, we might find $p < 0.05$ which suggests that we have found a significant result. However, this is not always true. Such errors are called Type I error (also known as False Positive). Other times, there might be a significant difference between our two groups but we might fail to identify this significance because we got $p \geq 0.05$. Such errors are called Type II errors (a.k.a. False Negative). Such errors usually occur when we don't have sufficient data (i.e. sample size is too small).


|**Error Type**| **Explanation** | **a.k.a.** |
|:--|:--|:--|
| **Type I** | $p < 0.05$ but in fact there is no significant difference  | False Positive|
| **Type II** | $p \geq 0.05$ but in fact there is a significant difference | False Negative|

### Type I Error
Remember that the difference between two groups of data is **statistically significant** means that there is an actual meaningful difference in the way these data are generated. This difference can be small or big in terms of its effect size. This difference might be caused by one factor or by many factors. What matters is that there is an underlying difference in the mechanism that is generating the data. This underlying difference is causing the actual variance in the data from two distinct groups. 

Also remember that the variance within or across two groups can also be caused by completely random factors. For example, when you are measuring the reaction time, a little insect in the room might lead to a bit of a distraction and increase the reaction time. 

Given all this background, let us see a little bit of a random data that was generated from a single distribution. 


```{r}
data_1 <- rnorm(10,mean =1, sd = 1)
data_2 <- rnorm(10,mean =1, sd = 1)

data_1

data_2

```


You see that the data points are different but the parameters are the same. So underlyingly, they come from the same distribution, same data generation process, there is no actual difference between the two datasets in terms of the underlying mechanism. 

Let us now pass the data through a t test for a few times to see if we can ever find a statistically significant difference. 


```{r}
set.seed(42)
t.test(rnorm(10,mean =1, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =1, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =1, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =1, sd = 1),rnorm(10,mean =1, sd = 1))

```


As you can see, the fourth model came out to be statistically significant. This is a Type I Error. 

### Typee II Error
Now, let us create data with different parameters. This time, the means are going to be different.

```{r}
data_1 <- rnorm(10,mean =0, sd = 1)
data_2 <- rnorm(10,mean =1, sd = 1)

data_1

data_2

```

Now, let's run the model a few times again. 

```{r}
set.seed(42)
t.test(rnorm(10,mean =0, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =0, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =0, sd = 1),rnorm(10,mean =1, sd = 1))
t.test(rnorm(10,mean =0, sd = 1),rnorm(10,mean =1, sd = 1))

```

As you can see, the results end up being not significant despite an actual difference in the parameters that generate data.


### Statistical Power
The rate at which a model would makes a Type II error is called Type II Error rate represented as $\beta$. We want to minimize $\beta$ as much as possible. The lower the error rate, the more powerful our model is. A standard way of measuring how powerful our model is the concept of **statistical power**. 

* Statistical power = 1-$\beta$

The higher the statistical power of a model, the more reliable it is. Often people aim for a value above 80%. To calculate the statistical power of a t test in r, we can use the following code. 

```{r}

effect_size <- cohen.d(rnorm(10,mean =0, sd = 1), rnorm(10,mean =1, sd = 1))$estimate
n <- 10
alpha <- 0.05

power <- power.t.test(n = n, delta = effect_size, sd = 1, sig.level = alpha, type = "two.sample")$power

power

```

Now, let us try the same data with an increased number of observations. 


```{r}

effect_size <- cohen.d(rnorm(10,mean =0, sd = 1), rnorm(10,mean =1, sd = 1))$estimate
n <- 300
alpha <- 0.05

power <- power.t.test(n = n, delta = effect_size, sd = 1, sig.level = alpha, type = "two.sample")$power

power

```

