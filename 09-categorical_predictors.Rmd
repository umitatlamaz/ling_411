# Categorical Predictors
Remember that we talked about four basic types of models in terms of their dependent and independent  variable types. Variables can assume **continuous** values (e.g. reaction time,  voicing duration, word count, etc.) or **categorical** values (e.g. positive-negative, gender, agreement type, word order (SVO - SOV) etc.).

|**Predictor Type (independent var.)**| **Outcome Type (dependent var.)** | 
|:--|:--|
| Continuous | Continuous | 
| **Categorical** | **Continuous** |
| Continuous | Categorical  |
| Categorical | Categorical  |

So far we built models where both the predictor and and the outcome are **continuous**. Now, we are moving to modeling data where the predictors are **categorical** and the outcomes are **continuous**. 

## Categorical Predictor - Continuous Outcome
Models with categorical predictors are quite common in linguistics as well as in many fields that rely on data analytics. Here are some examples:

* What are the reaction times of children vs. adults for a picture naming task?
* How does pro-drop impact the acceptability ratings of clauses. (Assuming ratings are continuous).
* How does NP ellipsis impact time to comprehend a linguistic expression?


Outside linguistics, especially in UX Research, people carry out a lot of A/B testing. For example, they check to see if a particular change to the UI has a significant impact on the user behavior. Here are some examples:

* What is the impact of the background color on the length of stay on a webpage?
* How much money do people from different cities spend on our platform?
* How does the language of a campaign affect the amount of donations made by people?

Categorical predictors are used whenever you compare two or more groups based on some classification (e.g. age, education level, native speaker status, and so on.).

## Taste vs. Smell Words
The data and analysis for this section comes from Bodo Winter's Chapter 7. 

Smell words have been claimed to be more negative than taste words. I don't know if this is true for every language or not but Turkish presents some very nice data points in this direction. Consider the following two expressions. 

* Burası (çok) kokuyor. 
* Bu (çok) tatlı.

Just the verb *kok* "smell" seems to have a negative connotation. On the other hand, the adjective *tatlı* "tasty"  whose root *tat*  "taste" and the suffix *-lı* simply means "with" has a positive connotation. While this is simply a hunch, we don't have enough evidence for Turkish to claim that this is in fact true. Let's hope that someone will run an experiment for Turkish and report the results. For now, we'll use the **senses_valence** dataset from Bodo Winter's book. 

Let us read in the dataset and see what it looks like. 

```{r message=FALSE}
#Import tidyverse
library(tidyverse)
#Read in the data
data <- read_csv('data/winter_2016_senses_valence.csv')
#print the head to see what it looks like
data
```

The dataset consists of three variables:

* **Word**: A word associated with some sense.
* **Modality**: Modality of the sense (touch, smell, etc.)
* **Valence**: A numeric value representing the attractiveness-aversiveness of a word. 
  * Higher Valence is better. See [Wikipedia for more on valence](https://en.wikipedia.org/wiki/Valence_(psychology)). 
  
For now, we're only interested in **smell** and **taste**. Yet, it looks like the data has more than that. Let's print the unique values in the Modality column to see all the categories. 
  
```{r}
unique(data$Modality)
```

Let's now select the rows that have only **smell** and **taste** values. For this, we will use the filter function. 

```{r message=FALSE}
# Filter the data
senses_data <- filter(data, Modality %in% c('Smell', 'Taste'))

# Check the unique values to make sure
unique(senses_data$Modality)
```

Let us quickly get some summary statistics using the `summarize()' function. 

```{r}
#pipe the data to a group_by function
#then pipe the groupings to the summarize function
# create the summary variables for mean and sd
senses_data %>% group_by(Modality) %>%
  summarize(M = mean(Val), SD = sd(Val))
```

It looks like the mean valence for the two groups (smell and taste) are slightly different. Without fitting a model, we won't yet know if this difference is significant (i.e. meaningful but not by chance). 

Before fitting a model though, let us visualize the data to see what it looks like. For categorical variables, it is often useful to plot a box-and-whiskers plot. 

```{r}
senses_data %>% ggplot(aes(x = Modality, y = Val, fill = Modality)) + 
  geom_boxplot()+
  theme_minimal()
```

An alternative way to plot the data is to use density graphs, which are essentially smoothed histograms. 

```{r}
senses_data %>% ggplot(aes(x = Val, fill = Modality, after_stat(scaled))) + 
  geom_density(alpha=0.5) 
```

## Contrasts & Coding
Linear models are essentially linear equations. This means that they are defined on numeric values (e.g. 5, 0.2, etc.) but not categorical values (e.g. child-adult, SOV-VSO, etc.). To be able to use linear models with categorical values, we need to convert our categories into some numeric values that can be used in a linear equation. This conversion of categorical values into numeric values is called **contrast coding**. There are various ways in which this coding can be done and they have slightly different interpretations. While numbers are somewhat arbitrary, the interpretation of the coefficients depends on the choice of the coding technique. 

### Treatment Coding.
One way of coding the difference between two categories is to convert them into ones and zeroes. This is called **treatment coding**. Sometimes it is also called **dummy coding**.

|**Word**| **Category** | **Treatment Coding** | 
|:--|:--|:--|
| odor | smell | 0 |
| sweet | taste | 1 |
| acrid | smell | 0 |


Let us do this by hand before training a model. 

```{r message=FALSE}
# Create a new column with 0 for smell and 1 for taste
senses_data <- mutate(senses_data, treatment = ifelse(Modality == 'Taste', 1, 0))

senses_data
```

Now that our categories are turned into numeric values, we can run a model. Let us fit a model where **valence** is a function of **modality** using our **treatment** codes.

```{r message=FALSE}
model_1 <- lm(Val ~ treatment, data = senses_data)

model_1
```

In treatment coding, the intercept becomes the mean of one of your variables whereas the slope is the difference between the two means. You can see this clearly once you plot a linear function between the two variables. 


```{r message=FALSE}
ggplot(senses_data, aes(x= treatment, y= Val))+
  scale_x_continuous(limits = c(-1,2)) +
  geom_point()+
  geom_smooth(method='lm', se=F)
```

Let us now take a look at the usual numbers R<sup>2</sup> and p-value to interpret how our model is doing. 

```{r}
summary(model_1)
```

Normally, you don't have to do the treatment coding by hand. R will do it automatically for you. We did it manually to make sure we understand what's under the hood. 

```{r}
model_2 <- lm(Val ~ Modality, data = senses_data)

model_2
```

**Interpreting the coefficients:** Our model is simply the following mathematical model.

$$valence = 5.8 + (−0.3) * modality$$
So, our model predicts only two values. 

```{r}
#Create a dataset consisting of categories in the data
preds <- tibble(Modality = unique(senses_data$Modality))
# Print to see what they look like
preds
# See the predictions
predict(model_2, preds)
```


### Sum Coding
Sum coding is a slightly different coding mechanism. Instead of using 0 and 1 as the coding scheme, we use -1 and 1 as the coding scheme. This has the benefit of having the mean of the means as the intercept. To be able to use R's coding functionality, we should convert our categorical values as **factors**. 

```{r}
senses_data<- mutate(senses_data, Modality = factor(Modality))

senses_data
```


Next, we can use the contrasts function to see what the current coding scheme looks like. 

```{r}
contrasts(senses_data$Modality)
```


We can also use R's built in cont function to get various coding types. 

**For treatment coding:**

```{r}
#Treatment coding with two variables
contr.treatment(2)
```


**For Sum coding:**

```{r}
#Sum coding with two variables
contr.sum(2)
```


**More than 2 variables:**

```{r}
#Treatment coding with 3 variables
contr.treatment(3)

#Sum coding with 3 variables
contr.sum(3)
```


**More than 2 variables:**

```{r}
#Treatment coding with 5 variables
contr.treatment(5)

#Sum coding with 5 variables
contr.sum(5)
```

Let us use sum coding on our data. 

```{r message=FALSE}
#Create a new column by copying the modality factors (Taste and Smell)
senses_data <- mutate(senses_data, sum_coding=Modality)
#Use sum coding using contrasts() and contr.sum()
contrasts(senses_data$sum_coding) <- contr.sum(2)
#run constrasts to see if it worked
contrasts(senses_data$sum_coding)
```


Now we can fit a linear model and see what the coefficients look like. 

```{r}
model_3 <- lm(Val ~ sum_coding, data=senses_data)

model_3
```

The intercept is now the mean of the means. The slope is halved. 


Let us also plot the model to see where the intercept is. 

```{r message=FALSE}
ggplot(senses_data, aes(x= sum_coding, y= Val))+
  geom_point()
```


## Categorical Predictors with more than 2 levels
Remember that originally our data had 5 levels ("Touch" "Sight" "Taste" "Smell" "Sound"). Let us build a linear model that includes all of them. 

```{r}
model_4 <- lm(Val ~ Modality, data=data)
model_4
```



```{r message=FALSE}

ggplot(data, aes(x= Modality, y= Val, color=Modality))+
  geom_point()+
  geom_smooth(method='lm', se=F)
```